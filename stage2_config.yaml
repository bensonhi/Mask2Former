# Stage 2 Configuration: Manual Annotations Fine-tuning (OPTIMIZED)
# Fine-tuning config for high-quality manual annotations
#
# OPTIMAL PARAMETERS from comprehensive 3-phase parameter search (22 experiments):
# - Best AP50: 36.88% on cropped test set (24 quadrants from 6 images)
# - Learning Rate: 0.00002 (2e-5) - optimal balance for convergence
# - Loss Weights: DICE=5.0, MASK=20.0, CLASS=2.0 - emphasizes overlapping structures
# - Training: 12,000 iterations sufficient (~430 epochs × 28 crops)
# - Augmentation: 0.7-1.4 scale range optimal
# - Loads weights from Stage 1 checkpoint

_BASE_: configs/coco/instance-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml

# Model configuration
MODEL:
  # Will be set dynamically to Stage 1 checkpoint
  WEIGHTS: ""  # Set by training script to stage1 checkpoint
  
  SEM_SEG_HEAD:
    # Single class: myotube  
    NUM_CLASSES: 1
  
  # Fine-tuned settings for precise manual annotations
  MASK_FORMER:
    # Inherit Swin-Large with 200 queries from Stage 1
    # OPTIMAL loss weights from parameter search (Phase 1: lr2e-5_dice - AP50=36.88%)
    DEEP_SUPERVISION: True
    CLASS_WEIGHT: 2.0    # Classification loss weight
    MASK_WEIGHT: 20.0    # OPTIMIZED: Reduced from 25.0 for better balance
    DICE_WEIGHT: 5.0     # OPTIMIZED: Increased from 2.0 - better for overlapping myotubes
    
    TEST:
      OBJECT_MASK_THRESHOLD: 0.3  # Slightly higher than Stage 1 for precision

# Dataset configuration for Stage 2 (manual annotations)
DATASETS:
  # PRODUCTION MODE (default): Train on all 10 base images, evaluate on same (for monitoring)
  TRAIN: ("myotube_stage2_train_full",)      # All 20 full images (10 base × 2 channels) - auto-cropped during training
  TEST: ("myotube_stage2_val_full_cropped",) # Same images as 80 quadrants (for monitoring convergence)

  # DEVELOPMENT MODE: Use 7/3 split for proper validation (uncomment to use)
  # TRAIN: ("myotube_stage2_train",)           # 14 full images (7 base × 2 channels) - auto-cropped during training
  # TEST: ("myotube_stage2_val_cropped",)      # 24 quadrants (3 base × 2 channels × 4 crops) for evaluation

# Stage 2 Solver: Fine-tuning Swin-Large with 4x training data (28 crops from 7 base images)
SOLVER:
  IMS_PER_BATCH: 1  # Swin-Large requires single batch even for fine-tuning
  BASE_LR: 0.00002  # OPTIMIZED: 2e-5 found optimal (tested 1e-5, 2e-5, 5e-5, 1e-4, 2e-4)
  MAX_ITER: 12000   # OPTIMIZED: 12k iterations sufficient (tested vs 18k, no improvement)
  # STEPS: (8400, 10800) # Not needed for cosine LR schedule
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500   # Extended warmup for stable convergence
  WEIGHT_DECAY: 0.01  # Reduced weight decay for fine-tuning
  OPTIMIZER: "ADAMW"
  LR_SCHEDULER_NAME: "WarmupCosineLR"  # Better convergence for fine-tuning
  BACKBONE_MULTIPLIER: 0.05  # Slightly higher - can learn more since Stage 1 is better
  CHECKPOINT_PERIOD: 1000  # Save checkpoints every evaluation period
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.005  # Slightly relaxed clipping for better adaptation
    NORM_TYPE: 2.0
  AMP:
    ENABLED: True

# Input configuration optimized for manual annotations
INPUT:
  IMAGE_SIZE: 1500  # Match original resolution for precise manual annotations
  MIN_SCALE: 0.7    # OPTIMIZED: Tested 0.5-0.8 range, 0.7-1.4 found optimal
  MAX_SCALE: 1.4    # OPTIMIZED: Tested up to 1.6, 0.7-1.4 achieves best balance
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "coco_instance_lsj"

  # CONSISTENT: Use same crop size as Stage 1 for transfer learning
  CROP:
    ENABLED: True
    TYPE: "relative_range"
    SIZE: [0.2, 0.3]  # Same as Stage 1 - important for consistent feature learning

  # STRONGER augmentations to combat overfitting on small dataset (7 base images)
  COLOR_AUG_SSD: True  # Color augmentation for fluorescence intensity variations

  RANDOM_FLIP: "horizontal"  # Horizontal flips (Note: detectron2 doesn't support "both" directly)

# Evaluation configuration
TEST:
  EVAL_PERIOD: 1000  # Evaluate every ~36 epochs (12000 iter ÷ 28 crops × 36/430 = 1000 iterations)
  AUG:
    ENABLED: False  # Disable TTA during training for speed (can enable for final eval)

# Data loader configuration - memory-optimized for Swin-Large
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4           # Conservative workers to avoid memory pressure during fine-tuning

# Output directory for Stage 2
OUTPUT_DIR: "./output_stage2_manual"

VERSION: 2 