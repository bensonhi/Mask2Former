# Stage 2 Configuration: Manual Annotations Fine-tuning
# Fine-tuning config for high-quality manual annotations
# 
# STAGE 2 DATASET: 7 images with high-quality manual annotations  
# - Training: 1400 iterations for precise fine-tuning (200 epochs × 7 images)
# - Optimized learning rate for effective adaptation to manual annotations
# - Loads weights from Stage 1 checkpoint

_BASE_: configs/coco/instance-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml

# Model configuration
MODEL:
  # Will be set dynamically to Stage 1 checkpoint
  WEIGHTS: ""  # Set by training script to stage1 checkpoint
  
  SEM_SEG_HEAD:
    # Single class: myotube  
    NUM_CLASSES: 1
  
  # Fine-tuned settings for precise manual annotations
  MASK_FORMER:
    # Inherit Swin-Large with 200 queries from Stage 1
    # Preserve Stage 1 loss weights (don't change what works)
    DEEP_SUPERVISION: True
    CLASS_WEIGHT: 2.0    # Keep Stage 1 settings
    MASK_WEIGHT: 25.0    # Keep Stage 1 settings  
    DICE_WEIGHT: 2.0     # Keep Stage 1 settings
    
    TEST:
      OBJECT_MASK_THRESHOLD: 0.3  # Slightly higher than Stage 1 for precision

# Dataset configuration for Stage 2 (manual annotations)
DATASETS:
  TRAIN: ("myotube_stage2_train",)
  TEST: ("myotube_stage2_val",)

# Stage 2 Solver: Fine-tuning Swin-Large with 4x training data (28 crops from 7 images)
SOLVER:
  IMS_PER_BATCH: 1  # Swin-Large requires single batch even for fine-tuning
  BASE_LR: 0.00002  # Conservative LR for fine-tuning large model
  MAX_ITER: 5000    # 28 crops × 180 epochs = 5,000 iterations
  # STEPS: (3500, 4500) # Not needed for cosine LR schedule
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 300   # Extended warmup for large model fine-tuning
  WEIGHT_DECAY: 0.01  # Reduced weight decay for fine-tuning
  OPTIMIZER: "ADAMW"
  LR_SCHEDULER_NAME: "WarmupCosineLR"  # Better convergence for fine-tuning
  BACKBONE_MULTIPLIER: 0.05  # Slightly higher - can learn more since Stage 1 is better
  CHECKPOINT_PERIOD: 1000  # Save checkpoints every evaluation period
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.005  # Slightly relaxed clipping for better adaptation
    NORM_TYPE: 2.0
  AMP:
    ENABLED: True
  # Memory optimization for Swin-Large
  FIND_UNUSED_PARAMETERS: False

# Input configuration optimized for manual annotations
INPUT:
  IMAGE_SIZE: 1500  # Match original resolution for precise manual annotations
  MIN_SCALE: 0.9    # Very conservative scaling for precise manual boundaries
  MAX_SCALE: 1.1    # Minimal scaling to preserve manual annotation precision
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "coco_instance_lsj"
  
  # CONSISTENT: Use same crop size as Stage 1 for transfer learning
  CROP:
    ENABLED: True
    TYPE: "relative_range"
    SIZE: [0.2, 0.3]  # Same as Stage 1 - important for consistent feature learning
  
  # Conservative augmentations to preserve manual annotation precision
  COLOR_AUG_SSD: True  # Mild color augmentation for fluorescence variations
  
  RANDOM_FLIP: "horizontal"  # Horizontal flips for myotube orientation diversity (same as Stage 1)

# Evaluation configuration  
TEST:
  EVAL_PERIOD: 1000  # Evaluate every 36 epochs (5000 iter ÷ 28 crops × 36/180 = 1000 iterations)

# Data loader configuration - memory-optimized for Swin-Large
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4           # Conservative workers to avoid memory pressure during fine-tuning

# Output directory for Stage 2
OUTPUT_DIR: "./output_stage2_manual"

VERSION: 2 