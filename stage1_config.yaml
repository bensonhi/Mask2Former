# Stage 1 Configuration: Algorithmic Annotations Training
# Based on Swin-Base config optimized for large dataset with algorithmic annotations
# 
# STAGE 1 DATASET: ~100 images with algorithmic annotations from batch processing
# - Training: 3000 iterations for robust feature learning
# - Conservative learning rate for stable training on noisy annotations
# - Focus on learning general myotube features

_BASE_: configs/coco/instance-segmentation/swin/maskformer2_swin_large_IN21k_384_bs16_100ep.yaml

# Model configuration
MODEL:
  # Use Swin-Large COCO pre-trained weights for initialization  
  WEIGHTS: "model_final_e5f453.pkl"
  
  SEM_SEG_HEAD:
    # Single class: myotube
    NUM_CLASSES: 1
  
  # Optimize for dense overlapping myotube structures
  MASK_FORMER:
    # Swin-Large already has 200 queries - perfect for dense myotube fields
    # Enhanced loss weights for overlapping structures
    DEEP_SUPERVISION: True
    CLASS_WEIGHT: 2.0    # Higher class weight for better myotube vs background
    MASK_WEIGHT: 25.0    # Higher mask weight for precise myotube boundaries
    DICE_WEIGHT: 2.0     # Dice loss for overlapping structures
    
    TEST:
      OBJECT_MASK_THRESHOLD: 0.25  # Lower threshold for thin/dim myotubes


# Dataset configuration for Stage 1 (algorithmic annotations)
DATASETS:
  TRAIN: ("myotube_stage1_train",)
  TEST: ("myotube_stage1_val",)

# Stage 1 Solver: Optimized for Swin-Large with 4x training data (360 crops from 90 images)
SOLVER:
  IMS_PER_BATCH: 2  # A40 48GB can handle 2 images with Swin-Large
  BASE_LR: 0.00002  # Optimized LR for batch size 2
  MAX_ITER: 9000    # 360 crops ร 25 epochs = 9,000 iterations
  # STEPS: (12600, 16200)  # Not needed for cosine LR schedule
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 1000   # Longer warmup for large model
  WEIGHT_DECAY: 0.05
  OPTIMIZER: "ADAMW"
  LR_SCHEDULER_NAME: "WarmupCosineLR"  # Better convergence than step decay
  BACKBONE_MULTIPLIER: 0.1  # Keep backbone learning conservative
  CHECKPOINT_PERIOD: 1000  # Less frequent checkpoints for longer training
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "full_model"
    CLIP_VALUE: 0.01
    NORM_TYPE: 2.0
  AMP:
    ENABLED: True

# Input configuration optimized for algorithmic annotations
INPUT:
  IMAGE_SIZE: 1500  # Match original image resolution for detail preservation
  MIN_SCALE: 0.8    # Conservative scaling to preserve thin myotube structures
  MAX_SCALE: 1.2    # Reduced range to avoid distorting elongated structures
  FORMAT: "RGB"
  DATASET_MAPPER_NAME: "coco_instance_lsj"
  
  # OPTIMIZED: Larger crops for better myotube connectivity context
  CROP:
    ENABLED: True
    TYPE: "relative_range" 
    SIZE: [0.2, 0.3]  # Larger crops to capture full myotube structures and overlaps
  
  # Enhanced augmentations for dense overlapping myotubes
  COLOR_AUG_SSD: True  # Helps with varying fluorescence intensity
  
  RANDOM_FLIP: "horizontal"  # Horizontal flips for myotube orientation diversity

# Evaluation configuration  
TEST:
  EVAL_PERIOD: 900  # Evaluate every 25 epochs (9000 iter รท 360 crops = 25 epochs)

# Data loader configuration - optimized for 52 CPUs + 1TB RAM
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 16          # Utilize more CPUs for faster data loading
  PIN_MEMORY: True         # Faster GPU transfer with 1TB RAM

# Output directory for Stage 1
OUTPUT_DIR: "./output_stage1_algorithmic"

VERSION: 2 